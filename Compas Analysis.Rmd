---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Compas Analysis

What follows are the calculations performed for ProPublica's analaysis of the COMPAS Recidivism Risk Scores. It might be helpful to open [the methodology](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm/) in another tab to understand the following.

## Loading the Data

We select fields for severity of charge, number of priors, demographics, age, sex, compas scores, and whether each person was accused of a crime within two years.

```{r jupyter={'outputs_hidden': False}}
library(dplyr)
library(ggplot2)
raw_data <- read.csv("./compas-scores-two-years.csv")
nrow(raw_data)
```

However not all of the rows are useable for the first round of analysis.

There are a number of reasons remove rows because of missing data:
* If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense.
* We coded the recidivist flag -- `is_recid` -- to be -1 if we could not find a compas case at all.
* In a similar vein, ordinary traffic offenses -- those with a `c_charge_degree` of 'O' -- will not result in Jail time are removed (only two of them).
* We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.

```{r jupyter={'outputs_hidden': False}}
df <- dplyr::select(raw_data, age, c_charge_degree, race, age_cat, score_text, sex, priors_count, 
                    days_b_screening_arrest, decile_score, is_recid, two_year_recid, c_jail_in, c_jail_out) %>% 
        filter(days_b_screening_arrest <= 30) %>%
        filter(days_b_screening_arrest >= -30) %>%
        filter(is_recid != -1) %>%
        filter(c_charge_degree != "O") %>%
        filter(score_text != 'N/A')
nrow(df)
```

Higher COMPAS scores are slightly correlated with a longer length of stay. 

```{r jupyter={'outputs_hidden': False}}
df$length_of_stay <- as.numeric(as.Date(df$c_jail_out) - as.Date(df$c_jail_in))
cor(df$length_of_stay, df$decile_score)
```

After filtering we have the following demographic breakdown:

```{r jupyter={'outputs_hidden': False}}
summary(df$age_cat)
```

```{r jupyter={'outputs_hidden': False}}
summary(df$race)
```

```{python jupyter={'outputs_hidden': False}}
print("Black defendants: %.2f%%" %            (3175 / 6172 * 100))
print("White defendants: %.2f%%" %            (2103 / 6172 * 100))
print("Hispanic defendants: %.2f%%" %         (509  / 6172 * 100))
print("Asian defendants: %.2f%%" %            (31   / 6172 * 100))
print("Native American defendants: %.2f%%" %  (11   / 6172 * 100))
```

```{r jupyter={'outputs_hidden': False}}
summary(df$score_text)
```

```{r jupyter={'outputs_hidden': False}}
xtabs(~ sex + race, data=df)
```

```{r jupyter={'outputs_hidden': False}}
summary(df$sex)
```

```{python jupyter={'outputs_hidden': False}}
print("Men: %.2f%%" %   (4997 / 6172 * 100))
print("Women: %.2f%%" % (1175 / 6172 * 100))
```

```{r jupyter={'outputs_hidden': False}}
nrow(filter(df, two_year_recid == 1))
```

```{r jupyter={'outputs_hidden': False}}
nrow(filter(df, two_year_recid == 1)) / nrow(df) * 100
```

Judges are often presented with two sets of scores from the Compas system -- one that classifies people into High, Medium and Low risk, and a corresponding decile score. There is a clear downward trend in the decile scores as those scores increase for white defendants.

```{r jupyter={'outputs_hidden': False}, magic_args="-w 900 -h 363 -u px"}
library(grid)
library(gridExtra)
pblack <- ggplot(data=filter(df, race =="African-American"), aes(ordered(decile_score))) + 
          geom_bar() + xlab("Decile Score") +
          ylim(0, 650) + ggtitle("Black Defendant's Decile Scores")
pwhite <- ggplot(data=filter(df, race =="Caucasian"), aes(ordered(decile_score))) + 
          geom_bar() + xlab("Decile Score") +
          ylim(0, 650) + ggtitle("White Defendant's Decile Scores")
grid.arrange(pblack, pwhite,  ncol = 2)
```

```{r jupyter={'outputs_hidden': False}}
xtabs(~ decile_score + race, data=df)
```

## Racial Bias in Compas

After filtering out bad rows, our first question is whether there is a significant difference in Compas scores between races. To do so we need to change some variables into factors, and run a logistic regression, comparing low scores to high scores.

```{r jupyter={'outputs_hidden': False}}
df <- mutate(df, crime_factor = factor(c_charge_degree)) %>%
      mutate(age_factor = as.factor(age_cat)) %>%
      within(age_factor <- relevel(age_factor, ref = 1)) %>%
      mutate(race_factor = factor(race)) %>%
      within(race_factor <- relevel(race_factor, ref = 3)) %>%
      mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%
      mutate(score_factor = factor(score_text != "Low", labels = c("LowScore","HighScore")))
model <- glm(score_factor ~ gender_factor + age_factor + race_factor +
                            priors_count + crime_factor + two_year_recid, family="binomial", data=df)
summary(model)
```

Black defendants are 45% more likely than white defendants to receive a higher score correcting for the seriousness of their crime, previous arrests, and future criminal behavior.

```{r jupyter={'outputs_hidden': False}}
control <- exp(-1.52554) / (1 + exp(-1.52554))
exp(0.47721) / (1 - control + (control * exp(0.47721)))
```

<!-- #region jupyter={"outputs_hidden": false} -->
Women are 19.4% more likely than men to get a higher score.
<!-- #endregion -->

```{r jupyter={'outputs_hidden': False}}
exp(0.22127) / (1 - control + (control * exp(0.22127)))
```

Most surprisingly, people under 25 are 2.5 times as likely to get a higher score as middle aged defendants.

```{r jupyter={'outputs_hidden': False}}
exp(1.30839) / (1 - control + (control * exp(1.30839)))
```

### Risk of Violent Recidivism

Compas also offers a score that aims to measure a persons risk of violent recidivism, which has a similar overall accuracy to the Recidivism score. As before, we can use a logistic regression to test for racial bias.

```{r jupyter={'outputs_hidden': False}}
raw_data <- read.csv("./compas-scores-two-years-violent.csv")
nrow(raw_data)
```

```{r jupyter={'outputs_hidden': False}}
df <- dplyr::select(raw_data, age, c_charge_degree, race, age_cat, v_score_text, sex, priors_count, 
                    days_b_screening_arrest, v_decile_score, is_recid, two_year_recid) %>% 
        filter(days_b_screening_arrest <= 30) %>%
        filter(days_b_screening_arrest >= -30) %>% 
        filter(is_recid != -1) %>%
        filter(c_charge_degree != "O") %>%
        filter(v_score_text != 'N/A')
nrow(df)
```

```{r jupyter={'outputs_hidden': False}}
summary(df$age_cat)
```

```{r jupyter={'outputs_hidden': False}}
summary(df$race)
```

```{r jupyter={'outputs_hidden': False}}
summary(df$v_score_text)
```

```{r jupyter={'outputs_hidden': False}}
nrow(filter(df, two_year_recid == 1)) / nrow(df) * 100
```

```{r jupyter={'outputs_hidden': False}}
nrow(filter(df, two_year_recid == 1))
```

```{r jupyter={'outputs_hidden': False}, magic_args="-w 900 -h 363 -u px"}
library(grid)
library(gridExtra)
pblack <- ggplot(data=filter(df, race =="African-American"), aes(ordered(v_decile_score))) + 
          geom_bar() + xlab("Violent Decile Score") +
          ylim(0, 700) + ggtitle("Black Defendant's Violent Decile Scores")
pwhite <- ggplot(data=filter(df, race =="Caucasian"), aes(ordered(v_decile_score))) + 
          geom_bar() + xlab("Violent Decile Score") +
          ylim(0, 700) + ggtitle("White Defendant's Violent Decile Scores")
grid.arrange(pblack, pwhite,  ncol = 2)
```

```{r jupyter={'outputs_hidden': False}}
df <- mutate(df, crime_factor = factor(c_charge_degree)) %>%
      mutate(age_factor = as.factor(age_cat)) %>%
      within(age_factor <- relevel(age_factor, ref = 1)) %>%
      mutate(race_factor = factor(race,
                                  labels = c("African-American", 
                                             "Asian",
                                             "Caucasian", 
                                             "Hispanic", 
                                             "Native American",
                                             "Other"))) %>%
      within(race_factor <- relevel(race_factor, ref = 3)) %>%
      mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%
      mutate(score_factor = factor(v_score_text != "Low", labels = c("LowScore","HighScore")))
model <- glm(score_factor ~ gender_factor + age_factor + race_factor +
                            priors_count + crime_factor + two_year_recid, family="binomial", data=df)
summary(model)
```

The violent score overpredicts recidivism for black defendants by 77.3% compared to white defendants.

```{r jupyter={'outputs_hidden': False}}
control <- exp(-2.24274) / (1 + exp(-2.24274))
exp(0.65893) / (1 - control + (control * exp(0.65893)))
```

Defendands under 25 are 7.4 times as likely to get a higher score as middle aged defendants.

```{r jupyter={'outputs_hidden': False}}
exp(3.14591) / (1 - control + (control * exp(3.14591)))
```

## Predictive Accuracy of COMPAS

In order to test whether Compas scores do an accurate job of deciding whether an offender is Low, Medium or High risk,  we ran a Cox Proportional Hazards model. Northpointe, the company that created COMPAS and markets it to Law Enforcement, also ran a Cox model in their [validation study](http://cjb.sagepub.com/content/36/1/21.abstract).

We used the counting model and removed people when they were incarcerated. Due to errors in the underlying jail data, we need to filter out 32 rows that have an end date more than the start date. Considering that there are 13,334 total rows in the data, such a small amount of errors will not affect the results.

```{r jupyter={'outputs_hidden': False}}
library(survival)
library(ggfortify)

data <- filter(filter(read.csv("./cox-parsed.csv"), score_text != "N/A"), end > start) %>%
        mutate(race_factor = factor(race,
                                  labels = c("African-American", 
                                             "Asian",
                                             "Caucasian", 
                                             "Hispanic", 
                                             "Native American",
                                             "Other"))) %>%
        within(race_factor <- relevel(race_factor, ref = 3)) %>%
        mutate(score_factor = factor(score_text)) %>%
        within(score_factor <- relevel(score_factor, ref=2))

grp <- data[!duplicated(data$id),]
nrow(grp)
```

```{r jupyter={'outputs_hidden': False}}
summary(grp$score_factor)
```

```{r jupyter={'outputs_hidden': False}}
summary(grp$race_factor)
```

```{r jupyter={'outputs_hidden': False}}
f <- Surv(start, end, event, type="counting") ~ score_factor
model <- coxph(f, data=data)
summary(model)
```

People placed in the High category are 3.5 times as likely to recidivate, and the COMPAS system's concordance 63.6%. This is lower than the accuracy quoted in the Northpoint study of 68%.

```{r jupyter={'outputs_hidden': False}}
decile_f <- Surv(start, end, event, type="counting") ~ decile_score
dmodel <- coxph(decile_f, data=data)
summary(dmodel)
```

COMPAS's decile scores are a bit more accurate at 66%.

We can test if the algorithm is behaving differently across races by including a race interaction term in the cox model.

```{r jupyter={'outputs_hidden': False}}
f2 <- Surv(start, end, event, type="counting") ~ race_factor + score_factor + race_factor * score_factor
model <- coxph(f2, data=data)
print(summary(model))
```

The interaction term shows a similar disparity as the logistic regression above.

High risk white defendants are 3.61 more likely than low risk white defendants, while High risk black defendants are 2.99 more likely than low.

```{python jupyter={'outputs_hidden': False}}
import math
print("Black High Hazard: %.2f" % (math.exp(-0.18976 + 1.28350)))
print("White High Hazard: %.2f" % (math.exp(1.28350)))
print("Black Medium Hazard: %.2f" % (math.exp(0.84286-0.17261)))
print("White Medium Hazard: %.2f" % (math.exp(0.84286)))
```

```{r jupyter={'outputs_hidden': False}, magic_args="-w 900 -h 563 -u px"}

fit <- survfit(f, data=data)

plotty <- function(fit, title) {
  return(autoplot(fit, conf.int=T, censor=F) + ggtitle(title) + ylim(0,1))
}
plotty(fit, "Overall")
```

Black defendants do recidivate at higher rates according to race specific Kaplan Meier plots.

```{r jupyter={'outputs_hidden': False}, magic_args="-w 900 -h 363 -u px"}
white <- filter(data, race == "Caucasian")
white_fit <- survfit(f, data=white)

black <- filter(data, race == "African-American")
black_fit <- survfit(f, data=black)

grid.arrange(plotty(white_fit, "White defendants"), 
             plotty(black_fit, "Black defendants"), ncol=2)
```

```{r jupyter={'outputs_hidden': False}}
summary(fit, times=c(730))
```

```{r jupyter={'outputs_hidden': False}}
summary(black_fit, times=c(730))
```

```{r jupyter={'outputs_hidden': False}}
summary(white_fit, times=c(730))
```

Race specific models have similar concordance values.

```{r jupyter={'outputs_hidden': False}}
summary(coxph(f, data=white))
```

```{r jupyter={'outputs_hidden': False}}
summary(coxph(f, data=black))
```

Compas's violent recidivism score has a slightly higher overall concordance score of 65.1%.

```{r jupyter={'outputs_hidden': False}}
violent_data <- filter(filter(read.csv("./cox-violent-parsed.csv"), score_text != "N/A"), end > start) %>%
        mutate(race_factor = factor(race,
                                  labels = c("African-American", 
                                             "Asian",
                                             "Caucasian", 
                                             "Hispanic", 
                                             "Native American",
                                             "Other"))) %>%
        within(race_factor <- relevel(race_factor, ref = 3)) %>%
        mutate(score_factor = factor(score_text)) %>%
        within(score_factor <- relevel(score_factor, ref=2))


vf <- Surv(start, end, event, type="counting") ~ score_factor
vmodel <- coxph(vf, data=violent_data)
vgrp <- violent_data[!duplicated(violent_data$id),]
print(nrow(vgrp))
summary(vmodel)
```

In this case, there isn't a significant coefficient on African American's with High Scores.

```{r jupyter={'outputs_hidden': False}}
vf2 <- Surv(start, end, event, type="counting") ~ race_factor + race_factor * score_factor
vmodel <- coxph(vf2, data=violent_data)
summary(vmodel)
```

```{r jupyter={'outputs_hidden': False}}
summary(coxph(vf, data=filter(violent_data, race == "African-American")))
```

```{r jupyter={'outputs_hidden': False}}
summary(coxph(vf, data=filter(violent_data, race == "Caucasian")))
```

```{r jupyter={'outputs_hidden': False}, magic_args="-w 900 -h 363 -u px"}
white <- filter(violent_data, race == "Caucasian")
white_fit <- survfit(vf, data=white)

black <- filter(violent_data, race == "African-American")
black_fit <- survfit(vf, data=black)

grid.arrange(plotty(white_fit, "White defendants"), 
             plotty(black_fit, "Black defendants"), ncol=2)
```

## Directions of the Racial Bias

The above analysis shows that the Compas algorithm does overpredict African-American defendant's future recidivism, but we haven't yet explored the direction of the bias. We can discover fine differences in overprediction and underprediction by comparing Compas scores across racial lines.

```{python jupyter={'outputs_hidden': False}}
from truth_tables import PeekyReader, Person, table, is_race, count, vtable, hightable, vhightable
from csv import DictReader

people = []
with open("./cox-parsed.csv") as f:
    reader = PeekyReader(DictReader(f))
    try:
        while True:
            p = Person(reader)
            if p.valid:
                people.append(p)
    except StopIteration:
        pass

pop = list(filter(lambda i: ((i.recidivist == True and i.lifetime <= 730) or
                              i.lifetime > 730), list(filter(lambda x: x.score_valid, people))))
recid = list(filter(lambda i: i.recidivist == True and i.lifetime <= 730, pop))
rset = set(recid)
surv = [i for i in pop if i not in rset]
```

```{python jupyter={'outputs_hidden': False}}
print("All defendants")
table(list(recid), list(surv))
```

```{python jupyter={'outputs_hidden': False}}
print("Total pop: %i" % (2681 + 1282 + 1216 + 2035))
```

```{python jupyter={'outputs_hidden': False}}
import statistics
print("Average followup time %.2f (sd %.2f)" % (statistics.mean(map(lambda i: i.lifetime, pop)),
                                                statistics.stdev(map(lambda i: i.lifetime, pop))))
print("Median followup time %i" % (statistics.median(map(lambda i: i.lifetime, pop))))
```

Overall, the false positive rate is 32.35%.

```{python jupyter={'outputs_hidden': False}}
print("Black defendants")
is_afam = is_race("African-American")
table(list(filter(is_afam, recid)), list(filter(is_afam, surv)))
```

That number is higher for African Americans at 44.85%.

```{python jupyter={'outputs_hidden': False}}
print("White defendants")
is_white = is_race("Caucasian")
table(list(filter(is_white, recid)), list(filter(is_white, surv)))
```

And lower for whites at 23.45%.

```{python jupyter={'outputs_hidden': False}}
44.85 / 23.45
```

Which means under COMPAS black defendants are 91% more likely to get a higher score and not go on to commit more crimes than white defendants after two year.


COMPAS scores misclassify white reoffenders as low risk at 70.4% more often than black reoffenders.

```{python jupyter={'outputs_hidden': False}}
47.72 / 27.99
```

```{python jupyter={'outputs_hidden': False}}
hightable(list(filter(is_white, recid)), list(filter(is_white, surv)))
```

```{python jupyter={'outputs_hidden': False}}
hightable(list(filter(is_afam, recid)), list(filter(is_afam, surv)))
```

<!-- #region jupyter={"outputs_hidden": true} -->
## Risk of Violent Recidivism

Compas also offers a score that aims to measure a persons risk of violent recidivism, which has a similar overall accuracy to the Recidivism score.
<!-- #endregion -->

```{python jupyter={'outputs_hidden': False}}
vpeople = []
with open("./cox-violent-parsed.csv") as f:
    reader = PeekyReader(DictReader(f))
    try:
        while True:
            p = Person(reader)
            if p.valid:
                vpeople.append(p)
    except StopIteration:
        pass

vpop = list(filter(lambda i: ((i.violent_recidivist == True and i.lifetime <= 730) or
                              i.lifetime > 730), list(filter(lambda x: x.vscore_valid, vpeople))))
vrecid = list(filter(lambda i: i.violent_recidivist == True and i.lifetime <= 730, vpeople))
vrset = set(vrecid)
vsurv = [i for i in vpop if i not in vrset]
```

```{python jupyter={'outputs_hidden': False}}
print("All defendants")
vtable(list(vrecid), list(vsurv))
```

Even moreso for Black defendants.

```{python jupyter={'outputs_hidden': False}}
print("Black defendants")
is_afam = is_race("African-American")
vtable(list(filter(is_afam, vrecid)), list(filter(is_afam, vsurv)))
```

```{python jupyter={'outputs_hidden': False}}
print("White defendants")
is_white = is_race("Caucasian")
vtable(list(filter(is_white, vrecid)), list(filter(is_white, vsurv)))
```

Black defendants are twice as likely to be false positives for a Higher violent score than white defendants.

```{python jupyter={'outputs_hidden': False}}
38.14 / 18.46
```

White defendants are 63% more likely to get a lower score and commit another crime than Black defendants.

```{python jupyter={'outputs_hidden': False}}
62.62 / 38.37
```

## Gender differences in Compas scores

In terms of underlying recidivism rates, we can look at gender specific Kaplan Meier estimates. There is a striking difference between women and men.

```{r jupyter={'outputs_hidden': False}}

female <- filter(data, sex == "Female")
male   <- filter(data, sex == "Male")
male_fit <- survfit(f, data=male)
female_fit <- survfit(f, data=female)
```

```{r jupyter={'outputs_hidden': False}}
summary(male_fit, times=c(730))
```

```{r jupyter={'outputs_hidden': False}}
summary(female_fit, times=c(730))
```

```{r jupyter={'outputs_hidden': False}, magic_args="-w 900 -h 363 -u px"}
grid.arrange(plotty(female_fit, "Female"), plotty(male_fit, "Male"),ncol=2)
```

As these plots show, the Compas score treats a High risk women the same as a Medium risk man.

```{python jupyter={'outputs_hidden': True}}

```
