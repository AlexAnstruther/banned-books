---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Compas Analysis


## Importing Libraries

```{python}
# %run -i projtools/imports.py
```

## Loading Data

```{python}
# %run -i projtools/data.py
```

## 'df' Descriptive Statistics

```{python}
df['length_of_stay'] = (pd.to_numeric(pd.to_datetime(df['c_jail_out'])) - pd.to_numeric(pd.to_datetime(df['c_jail_in'])))
df[['length_of_stay','decile_score']].corr().iloc[0,1]
```

```{python}
display(df['age_cat'].value_counts(),
    df['race'].value_counts(),
    df['race'].value_counts().apply(lambda x : x / len(df) * 100),
    df['score_text'].value_counts(),
    pd.crosstab(df['sex'],df['race']),
    df['sex'].value_counts(),
    df['sex'].value_counts().apply(lambda x : x / len(df) * 100),
    len(df[df['two_year_recid'] == 1]),
    len(df[df['two_year_recid'] == 1]) / len(df) * 100,
    pd.crosstab(df['decile_score'],df['race']))
```

```{python}
#using groupby to create a table which can be turned into a barchart
df['Index'] = df.index
african_american_barchart = df[df['race'] == 'African-American'].groupby(['decile_score'])['Index'].unique().apply(len)
caucasian_barchart = df[df['race'] == 'Caucasian'].groupby(['decile_score'])['Index'].unique().apply(len)
df = df.drop(['Index'],axis=1)
```

```{python}
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))

#format for barchart_setting(axes1,axes2,upper_ylim)
graph_formats.barchart_setting(ax1,ax2,620)

ax1.bar(african_american_barchart.index,african_american_barchart.values,color='dimgrey',zorder=2)
ax1.set_title("Black Defendent's Decile Scores")
ax2.bar(caucasian_barchart.index,caucasian_barchart.values,color='dimgrey',zorder=2)
ax2.set_title("White Defendent's Decile Scores");
```

```{python}
df['Index'] = df.index
african_american_barchart = df[df['race'] == 'African-American'].groupby(['decile_score'])['Index'].unique().apply(len)
caucasian_barchart = df[df['race'] == 'Caucasian'].groupby(['decile_score'])['Index'].unique().apply(len)
```

```{python}
df['Index'] = df.index

```

```{python}
model = smf.logit(formula='score_text_low_high_HighScore ~ sex_Female + age_cat_Less_than_25 + age_cat_Greater_than_45 + race_African_American + race_Asian + race_Hispanic + race_Native_American + race_Other + priors_count + c_charge_degree_M + two_year_recid', data=df).fit()
display(model.summary())
```

## Calculating Odds Ratios
Why did they do this so differently?
Does it matter - or can we back up what we are saying here?

```{python}
#converting log-odds to odds
odds_baselevel = np.exp(model.params.Intercept)
odds_black = np.exp(model.params.race_African_American)

#calculating an odds ratio
display(f"Using an odds ratio, black defendants are " + str(odds_black/odds_baselevel * 100)[:3] + "% more likely to receive a high score than white defendants.")
```

```{python}
odds_female = np.exp(model.params.sex_Female)
display(f"Using an odds ratio, female defendants are " + str(odds_female/odds_baselevel * 100)[:3] + "% more likely to receive a high score than male defendants.")
```

```{python}
odds_under25 = np.exp(model.params.age_cat_Less_than_25) / (1 + np.exp(model.params.age_cat_Less_than_25))
odds_over45 = np.exp(model.params.age_cat_Greater_than_45) / (1 + np.exp(model.params.age_cat_Greater_than_45))
display(f"Using an odds ratio, defendants under 25 are " + str(odds_under25/odds_baselevel * 100)[:3] + "% more likely to receive a high score than defendants between 25 and 45.")
display(f"Using an odds ratio, defendants over 45 are " + str(odds_over45/odds_baselevel * 100)[:2] + "% more likely to receive a high score than defendants between 25 and 45.")

```

## 'violent_df' Descriptive Statistics

```{python}
display(violent_df['age_cat'].value_counts(),
    violent_df['race'].value_counts(),
    violent_df['v_score_text'].value_counts(),
    len(violent_df[violent_df['two_year_recid'] == 1]),
    len(violent_df[violent_df['two_year_recid'] == 1]) / len(violent_df) * 100)
```

```{python}
violent_df['Index'] = violent_df.index
v_african_american_barchart = violent_df[violent_df['race'] == 'African-American'].groupby(['v_decile_score'])['Index'].unique().apply(len)
v_caucasian_barchart = violent_df[violent_df['race'] == 'Caucasian'].groupby(['v_decile_score'])['Index'].unique().apply(len)
violent_df = violent_df.drop(['Index'],axis=1)
```

```{python}
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))

graph_formats.barchart_setting(ax1,ax2,700)

ax1.bar(v_african_american_barchart.index,v_african_american_barchart.values,color='dimgrey',zorder=2)
ax1.set_title("Black Defendent's Violent Decile Scores")
ax2.bar(v_caucasian_barchart.index,v_caucasian_barchart.values,color='dimgrey',zorder=2)
ax2.set_title("White Defendent's Violent Decile Scores");
```

```{python}
model = smf.logit(formula='v_score_text_low_high_HighScore ~ sex_Female + age_cat_Less_than_25 + age_cat_Greater_than_45 + race_African_American + race_Asian + race_Hispanic + race_Native_American + race_Other + priors_count + c_charge_degree_M + two_year_recid', data=violent_df).fit()
print(model.summary())
```

## 'cox_df' Descriptive Statistics

```{python}
display(cox_df['score_text'].value_counts(),
    cox_df['race'].value_counts())
```

## Predictive Accuracy of COMPAS

```{python}
from itertools import combinations
recid_by_size = cox_df.groupby('score_text')['is_recid'].agg(['sum','size'])
recid_by_size.loc['HighMedium'] = recid_by_size.loc['High'] + recid_by_size.loc['Medium']
recid_by_size['percent'] = recid_by_size['sum']/recid_by_size['size']
display(f"HighMedium vs Low: {recid_by_size.loc['HighMedium','percent']/recid_by_size.loc['Low','percent']}",
    f"High vs Low: {recid_by_size.loc['High','percent']/recid_by_size.loc['Low','percent']}",
    f"Text concordance: {conc.concordance_text_score(cox_df,['is_recid','score_text'],100)}")
```

```{python}
display(f"Decile concordance: {conc.concordance_decile_score(cox_df,['is_recid','decile_score'],100)}")
```

```{python}
recid_by_size_black = cox_df[cox_df['race'] == 'African-American'].groupby('score_text')['is_recid'].agg(['sum','size'])
recid_by_size_black['percent'] = recid_by_size_black['sum']/recid_by_size_black['size']
recid_by_size_white = cox_df[cox_df['race'] == 'Caucasian'].groupby('score_text')['is_recid'].agg(['sum','size'])
recid_by_size_white['percent'] = recid_by_size_white['sum']/recid_by_size_white['size']
display(f"High vs Low (African-American): {recid_by_size_black.loc['High','percent']/recid_by_size_black.loc['Low','percent']}",
        f"High vs Low (Caucasian): {recid_by_size_white.loc['High','percent']/recid_by_size_white.loc['Low','percent']}")
```

```{python}
for score, color in zip(['Low', 'Medium', 'High'], ['green', 'orange', 'red']):
    subset = cox_df[(cox_df['is_recid'] == 1) & (cox_df['score_text'] == score)]
    total_count = len(cox_df[cox_df['score_text'] == score])
    value_counts = subset['duration'].value_counts().sort_index().cumsum()
    proportion = (total_count - value_counts) / total_count
    proportion.plot(color=color,
                    label=score,
                    title='Entire Sample',
                    ylim=[0,1],
                    ylabel='Proportion Not Recidivised',
                    xlabel='Days',
                    legend=True,
                    grid=True)
```

```{python}
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
fig.suptitle('Non-violent recidivism by race', fontsize=16)
cox_df_black = cox_df[cox_df['race'] == 'African-American']
cox_df_white = cox_df[cox_df['race'] == 'Caucasian']

for score, color in zip(['Low', 'Medium', 'High'], ['green', 'orange', 'red']):
    subset = cox_df_white[(cox_df_white['is_recid'] == 1) & (cox_df_white['score_text'] == score)]
    total_count = len(cox_df_white[cox_df_white['score_text'] == score])
    value_counts = subset['duration'].value_counts().sort_index().cumsum()
    proportion = (total_count - value_counts) / total_count
    proportion.plot(ax=ax1, color=color, label=score, title='White')

ax1.set_ylim([0, 1])
ax1.set_ylabel('Proportion Not Recidivised')
ax1.set_xlabel('Days')
ax1.legend()
ax1.grid()

for score, color in zip(['Low', 'Medium', 'High'], ['green', 'orange', 'red']):
    subset = cox_df_black[(cox_df_black['is_recid'] == 1) & (cox_df_black['score_text'] == score)]
    total_count = len(cox_df_black[cox_df_black['score_text'] == score])
    value_counts = subset['duration'].value_counts().sort_index().cumsum()
    proportion = (total_count - value_counts) / total_count
    proportion.plot(ax=ax2, color=color, label=score, title='Black')

ax2.set_ylim([0, 1])
ax2.set_ylabel('Proportion Not Recidivised')
ax2.set_xlabel('Days')
ax2.legend(loc='lower left')
ax2.grid()
```

```{python}
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
fig.suptitle('Violent recidivism by race', fontsize=16)
cox_df_violent_black = cox_df_violent[cox_df_violent['race'] == 'African-American']
cox_df_violent_white = cox_df_violent[cox_df_violent['race'] == 'Caucasian']
cox_df_violent['duration']

for score, color in zip(['Low', 'Medium', 'High'], ['green', 'orange', 'red']):
    subset = cox_df_violent_white[(cox_df_violent_white['is_recid'] == 1) & (cox_df_violent_white['score_text'] == score)]
    total_count = len(cox_df_violent_white[cox_df_violent_white['score_text'] == score])
    value_counts = subset['duration'].value_counts().sort_index().cumsum()
    proportion = (total_count - value_counts) / total_count
    proportion.plot(ax=ax1, color=color, label=score, title='White')

ax1.set_ylim([0, 1])
ax1.set_ylabel('Proportion Not Recidivised')
ax1.set_xlabel('Days')
ax1.legend()
ax1.grid()

for score, color in zip(['Low', 'Medium', 'High'], ['green', 'orange', 'red']):
    subset = cox_df_violent_black[(cox_df_violent_black['is_recid'] == 1) & (cox_df_violent_black['score_text'] == score)]
    total_count = len(cox_df_violent_black[cox_df_violent_black['score_text'] == score])
    value_counts = subset['duration'].value_counts().sort_index().cumsum()
    proportion = (total_count - value_counts) / total_count
    proportion.plot(ax=ax2, color=color, label=score, title='Black')

ax2.set_ylim([0, 1])
ax2.set_ylabel('Proportion Not Recidivised')
ax2.set_xlabel('Days')
ax2.legend(loc='lower left')
ax2.grid()
```

```{python}
display(f"Violent Text concordance: {conc.concordance_text_score(cox_df_violent,['is_recid','score_text'],100)}")
```

# Directions of the Racial Bias


Realised what is_recid is vs event - is_recid is whether someone has recidivised while event only increases to one, once per person. This means is_recid can have multiple values for the same person when they have only recidivised once

```{python}
#create class called person
df = pd.read_csv('CompasAnalysis/cox-parsed.csv')
```

```{python}
#this finds the total lifetime per person
lifetimes =  df.groupby('id')[['start','end']].sum().apply(lambda x: x['end'] - x['start'], axis=1)

#removing duplicate ID rows
df = df[~df['id'].duplicated(keep='first')]

df['lifetime'] = lifetimes.values

#if recidivist filter lifetimes so only contains people under two years - otherwise contain people who did not recidivise for over two years
df = df[((df['lifetime'] <= 730) & (df['is_recid'] == 1)) | (df['lifetime'] > 730)]
```

```{python}
#creating my own confusion matrices
is_recid = (df['is_recid'] == 1) & (df['lifetime'] <= 730)
is_not_recid = (df['lifetime'] > 730)
score_high = (df['score_text'] == 'High') | (df['score_text'] == 'Medium')
score_low = df['score_text'] == 'Low'
df['truth_table_recid'] = np.select([is_not_recid, is_recid], ['Survivor','Recidivist'], default=np.nan)
df['truth_table_score'] = np.select([score_low, score_high], ['Survivor','Recidivist'], default=np.nan)
```

```{python}
cm_df = df[['id','race','truth_table_recid','truth_table_score']]
cm_df = cm_df.dropna(subset=['truth_table_recid','truth_table_score'])
cm = confusion_matrix(y_true=cm_df['truth_table_recid'],y_pred=cm_df['truth_table_score'],labels=['Survivor','Recidivist'])
fig = ConfusionMatrixDisplay(cm,display_labels=['Survivor','Recidivist'])
fig.plot(cmap=plt.cm.Blues)
plt.text(-0.4,-0.6,'Confusion Matrix - All Defendants', fontdict={'weight': 'bold', 'size': 16});
```

```{python}
print(f"Total pop: {(2681 + 1282 + 1216 + 2035)}")
print(f"Overall the false positive rate is {1282/(2681 + 1282)}")
#there are slightly different survivor figures compared to pro publica
```

```{python}
display(f"The mean time between release and re-entry of custody is {np.mean(df['lifetime'])} days (stdev: {np.std(df['lifetime'])} days.)")
display(f"The median time between release and re-entry of custody is {np.median(df['lifetime'])} days.")
#different figures again
```
=======
df.columns
```

```{python}
 fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
fig.suptitle('Violent recidivism by race', fontsize=16)
df_men = df[df['sex'] == 'Male']
df_women = df[df['sex'] == 'Female']

for score, color in zip(['Low', 'Medium', 'High'], ['green', 'orange', 'red']):
    subset = df_women[(df_women['is_recid'] == 1) & (df_women['score_text'] == score)]
    total_count = len(df_women[df_women['score_text'] == score])
    value_counts = subset['duration'].value_counts().sort_index().cumsum()
    proportion = (total_count - value_counts) / total_count
    proportion.plot(ax=ax1, color=color, label=score, title='Women')

ax1.set_ylim([0, 1])
ax1.set_ylabel('Proportion Not Recidivised')
ax1.set_xlabel('Days')
ax1.legend()
ax1.grid()

for score, color in zip(['Low', 'Medium', 'High'], ['green', 'orange', 'red']):
    subset = df_men[(df_men['is_recid'] == 1) & (df_men['score_text'] == score)]
    total_count = len(df_men[cox_df_violent_black['score_text'] == score])
    value_counts = subset['duration'].value_counts().sort_index().cumsum()
    proportion = (total_count - value_counts) / total_count
    proportion.plot(ax=ax2, color=color, label=score, title='men')

ax2.set_ylim([0, 1])
ax2.set_ylabel('Proportion Not Recidivised')
ax2.set_xlabel('Days')
ax2.legend(loc='lower left')
ax2.grid()
```

```{python}
black_cm_df = cm_df[cm_df['race'] == 'African-American']
cm = confusion_matrix(y_true=black_cm_df['truth_table_recid'],y_pred=black_cm_df['truth_table_score'],labels=['Survivor','Recidivist'])
fig = ConfusionMatrixDisplay(cm,display_labels=['Survivor','Recidivist'])
fig.plot(cmap=plt.cm.Blues)
plt.text(-0.55,-0.6,'Confusion Matrix - Black Defendants', fontdict={'weight': 'bold', 'size': 16});
```

```{python}
print(f"For black defendants, the false positive rate is {805/(990 + 805)}")
```

```{python}
white_cm_df = cm_df[cm_df['race'] == 'Caucasian']
cm = confusion_matrix(y_true=white_cm_df['truth_table_recid'],y_pred=white_cm_df['truth_table_score'],labels=['Survivor','Recidivist'])
fig = ConfusionMatrixDisplay(cm,display_labels=['Survivor','Recidivist'])
fig.plot(cmap=plt.cm.Blues)
plt.text(-0.55,-0.6,'Confusion Matrix - White Defendants', fontdict={'weight': 'bold', 'size': 16});
```

```{python}
print(f"For white defendants, the false positive rate is {349/(349 + 1139)}")
print(f"Therefore, black defendants are {round(((805/(990 + 805))/(349/(349 + 1139)) - 1) * 100,2)}% more likely to receive a High COMPAS score and not recidivise within 2 years (false \npositive) than white defendants.")
print(f"Furthermore, white defendants are {round(((461/(461 + 505))/(532/(532 + 1369)) - 1) * 100,2)}% more likely to receive a Low COMPAS score yet still recidivise within 2 years (false \nnegative) than black defendants.")
```

## Risk of Violent Recidivism

```{python}
#reading in cox-violent-parsed
v_df = pd.read_csv('CompasAnalysis/cox-violent-parsed.csv')   
```

```{python}
#this finds the total lifetime per person
lifetimes_df =  pd.DataFrame(v_df.groupby('id')[['start','end']].sum().apply(lambda x: x['end'] - x['start'], axis=1)).rename(columns={0:'lifetime'})
lifetimes = lifetimes_df['lifetime']

#removing duplicate ID rows
v_df = v_df[~v_df['id'].duplicated(keep='first')]

v_df['lifetime'] = lifetimes

#if recidivist filter lifetimes so only contains people under two years - otherwise contain people who did not recidivise for over two years
v_df = v_df[((v_df['lifetime'] <= 730) & (v_df['is_violent_recid'] == 1)) | (v_df['lifetime'] > 730)]
```

```{python}
#creating conditions to define the scores someone gets in the confusion matrix
#in reality recidivised within two years
is_recid = (v_df['is_violent_recid'] == 1) & (v_df['lifetime'] <= 730)
#in reality did not recidivise within two years
is_not_recid = (v_df['lifetime'] > 730)
#predicted to recidivise
score_high = (v_df['v_score_text'] == 'High') | (v_df['v_score_text'] == 'Medium')
#predicted to not recidivise
score_low = v_df['v_score_text'] == 'Low'

#applying the conditions to create new prediction and true columns with survivor and recidivist values
v_df['truth_table_recid'] = np.select([is_not_recid, is_recid], ['Survivor','Recidivist'], default=np.nan)
v_df['truth_table_score'] = np.select([score_low, score_high], ['Survivor','Recidivist'], default=np.nan)
```

```{python}
#creating smaller df for our confusion matrices
v_cm_df = v_df[['id','race','truth_table_recid','truth_table_score']]
v_cm_df = v_cm_df.dropna(subset=['truth_table_recid','truth_table_score'])

#setting up confusion matrix
cm = confusion_matrix(y_true=v_cm_df['truth_table_recid'],y_pred=v_cm_df['truth_table_score'],labels=['Survivor','Recidivist'])
fig = ConfusionMatrixDisplay(cm,display_labels=['Survivor','Recidivist'])
fig.plot(cmap=plt.cm.Blues)
plt.text(-0.2,-0.6,'All Defendants - Violent', fontdict={'weight': 'bold', 'size': 16});
```

```{python}
print(f"Overall for violent recidivism, the false positive rate is {1088/(1088 + 2350)}")
```

```{python}
v_black_cm_df = v_cm_df[v_cm_df['race'] == 'African-American']
cm = confusion_matrix(y_true=v_black_cm_df['truth_table_recid'],y_pred=v_black_cm_df['truth_table_score'],labels=['Survivor','Recidivist'])
fig = ConfusionMatrixDisplay(cm,display_labels=['Survivor','Recidivist'])
fig.plot(cmap=plt.cm.Blues)
plt.text(-0.25,-0.6,'Black Defendants - Violent', fontdict={'weight': 'bold', 'size': 16});
```

```{python}
print(f"For violent recidivism in black defendants, the false positive rate is {743/(743 + 974)}")
```

```{python}
v_white_cm_df = v_cm_df[v_cm_df['race'] == 'Caucasian']
cm = confusion_matrix(y_true=v_white_cm_df['truth_table_recid'],y_pred=v_white_cm_df['truth_table_score'],labels=['Survivor','Recidivist'])
fig = ConfusionMatrixDisplay(cm,display_labels=['Survivor','Recidivist'])
fig.plot(cmap=plt.cm.Blues)
plt.text(-0.2,-0.6,'White Defendants - Violent', fontdict={'weight': 'bold', 'size': 16});
```

```{python}
print(f"For violent recidivism in white defendants, the false positive rate is {243/(243 + 963)}")
print(f"Therefore, black defendants are {round(((743/(743 + 974))/(243/(243 + 963)) - 1) * 100,2)}% more likely to receive a High Violence COMPAS score and not violently recidivise \nwithin 2 years (false positive) than white defendants.")
print(f"Furthermore, white defendants are {round(((48/(48 + 23))/(53/(53 + 95)) - 1) * 100,2)}% more likely to receive a Low Violence COMPAS score yet still violently recidivise \nwithin 2 years (false negative) than black defendants.")
```

# Extension
Ideas:  
- Investigate the impact of relationship status on COMPAS score and recidivism rates
- Investigate the impact of juvenile priors and normal priors
- Use VAD values of c_charge_desc as an input into a regression equation for calculating whether someone recidivises or not


# Investigating Prior Charges
Can these be used to predict how soon a defendant will recidivise, whether they will recidivise and their COMPAS score?

```{python}
df = pd.read_csv('CompasAnalysis/cox-parsed.csv')

#this finds the total lifetime per person
lifetimes =  df.groupby('id')[['start','end']].sum().apply(lambda x: x['end'] - x['start'], axis=1)

#removing duplicate ID rows
df = df[~df['id'].duplicated(keep='first')]

df['lifetime'] = lifetimes.values
#only taking people who recidivised within the first two years
df = df[((df['is_recid'] == 1) & (df['lifetime'] <= 730)) | ((df['is_recid'] == 0) & (df['lifetime'] > 730))]
```

## Using Prior Convictions to Create a Lifetime Model
Lifetime is the number of between someone's release for their original case and the date they entered jail for their recidivised case.  
For non-recidivists, it is the time they have been released since their original case.

```{python}
#using prior convictions to predict someone's lifetime
prior_lifetime_model = smf.ols(formula='lifetime ~ juv_fel_count + juv_misd_count + juv_other_count + priors_count', data=df).fit()
prior_lifetime_model.summary()
```

```{python}
print(f"From this linear regression, we can understand that individuals without any prior convictions are anticipated to on average have a lifetime of {round(prior_lifetime_model.params.Intercept,2)} days.")
print(f"\nFor each of these types of prior conviction, defendants tend to recidivise sooner. \nThe figures below show the change for each extra instance: \nJuvenile Felonies: {round(abs(prior_lifetime_model.params.juv_fel_count),2)} days sooner\nJuvenile Other: {round(abs(prior_lifetime_model.params.juv_other_count),2)} days sooner\nPriors: {round(abs(prior_lifetime_model.params.priors_count),2)} days sooner\nJuvenile Misdemeanours are not significant")
```

## Using Prior Convictions to Create a COMPAS Decile Score Model

```{python}
#were prior convictions used to calculate decile scores
prior_decile_model = smf.ols(formula='decile_score ~ juv_fel_count + juv_misd_count + juv_other_count + priors_count', data=df).fit()
prior_decile_model.summary()
```

```{python}
print(f"From this linear regression, we can understand that individuals without any prior convictions are anticipated to on average have a decile score of {round(prior_decile_model.params.Intercept,2)}")
print(f"\nFor each of these types of prior conviction, defendants tend to have a higher score.\nThe figures below show the change for each extra instance: \nJuvenile Felonies: {round(abs(prior_decile_model.params.juv_fel_count),2)} points higher",
      f"\nJuvenile Misdemeanours: {round(abs(prior_decile_model.params.juv_misd_count),2)} points higher",
      f"\nJuvenile Other: {round(abs(prior_decile_model.params.juv_other_count),2)} points higher",
      f"\nPriors: {round(abs(prior_decile_model.params.priors_count),2)} points higher")
print("\nThis indicates that all of these data-inputs were likely used as part of the algorithm which constructs COMPAS Decile Scores")
```

## Using Prior Convictions to Create a Recidivism Model

```{python}
#do prior convictions actually influence recidivism?
prior_recid_model = smf.logit(formula='is_recid ~ juv_fel_count + juv_misd_count + juv_other_count + priors_count', data=df).fit()
prior_recid_model.summary()
```

### Calculating Odds Ratios

```{python}
odds_baselevel = np.exp(prior_recid_model.params.Intercept)
print(f"According to the logistic regression, on average: \nA defendant with one juvenile felony is {round((np.exp(prior_recid_model.params.juv_fel_count)/odds_baselevel - 1)* 100,1)}% more likely to be a recidivist than a defendant with none.",
     f"\nA defendant with one juvenile misdemeanour is {round((np.exp(prior_recid_model.params.juv_misd_count)/odds_baselevel - 1)* 100,1)}% more likely to be a recidivist than a defendant with none.",
     f"\nA defendant with one other juvenile prior conviction is {round((np.exp(prior_recid_model.params.juv_other_count)/odds_baselevel - 1)* 100,1)}% more likely to be a recidivist than a defendant with none.",
     f"\nA defendant with one prior conviction is {round((np.exp(prior_recid_model.params.priors_count)/odds_baselevel - 1)* 100,1)}% more likely to be a recidivist than a defendant with none.",)
```

### Further Analysis

```{python}
df['predicted_recid'] = df['juv_fel_count'] * prior_recid_model.params.juv_fel_count + df['juv_misd_count'] * prior_recid_model.params.juv_misd_count + df['juv_other_count'] * prior_recid_model.params.juv_other_count + df['priors_count'] * prior_recid_model.params.priors_count + prior_recid_model.params.Intercept
plt.hist2d(df['predicted_recid'],df['is_recid'],bins=(20,2),cmap='Blues', norm=LogNorm())
plt.yticks([0,1])
plt.xlabel('Logistic Regression Prediction')
plt.ylabel('Recidivist')
plt.title('Recidivist Prediction vs Actual', fontdict={'weight':'bold', 'size':16})
cbar = plt.colorbar()
```

```{python}
from matplotlib.lines import Line2D
df['predicted_recid'] = df['juv_fel_count'] * prior_recid_model.params.juv_fel_count + df['juv_misd_count'] * prior_recid_model.params.juv_misd_count + df['juv_other_count'] * prior_recid_model.params.juv_other_count + df['priors_count'] * prior_recid_model.params.priors_count + prior_recid_model.params.Intercept
recid_data = df[df['is_recid'] == 1]['predicted_recid']
nonrecid_data = df[df['is_recid'] == 0]['predicted_recid']
#plt.hist2d(df['predicted_recid'],df['is_recid'],bins=(20,2),cmap='Blues', norm=LogNorm())
plt.boxplot([nonrecid_data,recid_data],labels=['Non-Recidivists','Recidivists'], flierprops=dict(marker='x', markeredgecolor='black', markerfacecolor='red', markersize=1), showmeans=True, meanline=True)
plt.xlabel('')
plt.ylabel('Predicted Recidivism Score')
plt.title('Recidivist Prediction vs Actual', fontdict={'weight':'bold', 'size':16})
legend_elements = [
    Line2D([0], [0], marker='s', color='orange', markerfacecolor='orange', markersize=10, label='Median'),
    Line2D([0], [0], marker='s', color='w', markerfacecolor='green', markersize=10, label='Mean')
]
plt.legend(handles=legend_elements, loc='upper left')
#cbar = plt.colorbar()
```

```{python}
print(f"Prediction STDEV for Recidivists: {np.std(df[df['is_recid'] == 1]['predicted_recid'])}",
      f"\nPrediction STDEV for Non-Recidivists: {np.std(df[df['is_recid'] == 0]['predicted_recid'])}",
      f"\nPrediction Mean for Recidivists: {np.mean(df[df['is_recid'] == 1]['predicted_recid'])}",
      f"\nPrediction Mean for Non-Recidivists: {np.mean(df[df['is_recid'] == 0]['predicted_recid'])}",)
```

# NLP Stuff

```{python}
vad_df = pd.DataFrame(pd.read_csv("data/NRC-VAD-Lexicon.txt")['aaaaaaah\t0.479\t0.606\t0.291'].str.split('\t'))
vad_df = vad_df.rename(columns={'aaaaaaah\t0.479\t0.606\t0.291':'Words'})

#splitting up list into multiple columns
vad_df['Valence'] = vad_df['Words'].apply(lambda x: x[1])
vad_df['Arousal'] = vad_df['Words'].apply(lambda x: x[2])
vad_df['Dominance'] = vad_df['Words'].apply(lambda x: x[3])
vad_df['Words'] = vad_df['Words'].apply(lambda x: x[0])
```

```{python}
#converting the dataframe to dictionaries
valence_dict = vad_df.set_index('Words')['Valence'].astype('float').to_dict()
arousal_dict = vad_df.set_index('Words')['Arousal'].astype('float').to_dict()
dominance_dict = vad_df.set_index('Words')['Dominance'].astype('float').to_dict()
```

```{python}
#creating a list of formatted charge lemmas
nlp = spacy.load('data/en_core_web_sm/en_core_web_sm-3.7.1')
def remove_punctuation(x):
    punctuation_trans = str.maketrans("", "", string.punctuation.join('/w'))
    # Use translate method to remove punctuation
    final_string = x.translate(punctuation_trans)
    return final_string
def lemmatizer(x):
    processed = nlp(x)
    lemmatized_string = ' '.join([token.lemma_ for token in processed])
    return lemmatized_string
df['c_charge_desc'] = df['c_charge_desc'].astype('str').apply(remove_punctuation)
df['lemmatized_charges'] = df['c_charge_desc'].astype('str').apply(lemmatizer)
```

```{python}
charges_df = pd.DataFrame()
charges_df['Charges'] = df['lemmatized_charges'].value_counts().index.str.split()
```

```{python}
charges_df = pd.DataFrame()
charges_df['Charges'] = df['lemmatized_charges'].value_counts().index.str.split()
```

```{python}
charges_df['Charges']
```

```{python}
def valence_average(x):
    scores = []
    for i in range(len(x)):
        #takes the uncapitalised version and retrieves its value from the dictionary
        scores.append(valence_dict.get(x[i].lower(), np.nan))
    #removing NaNs so that the length corresponds to the number of values
    scores = [score for score in scores if not math.isnan(score)]
    if len(scores) == 0:
        return np.nan
    return sum(scores)/len(scores)
def arousal_average(x):
    scores = []
    for i in range(len(x)):
        #takes the uncapitalised version and retrieves its value from the dictionary
        scores.append(arousal_dict.get(x[i].lower(), np.nan))
    #removing NaNs so that the length corresponds to the number of values
    scores = [score for score in scores if not math.isnan(score)]
    if len(scores) == 0:
        return np.nan
    return sum(scores)/len(scores)
def dominance_average(x):
    scores = []
    for i in range(len(x)):
        #takes the uncapitalised version and retrieves its value from the dictionary
        scores.append(dominance_dict.get(x[i].lower(), np.nan))
    #removing NaNs so that the length corresponds to the number of values
    scores = [score for score in scores if not math.isnan(score)]
    if len(scores) == 0:
        return np.nan
    return sum(scores)/len(scores)
charges_df['Valence'] = charges_df['Charges'].apply(valence_average)
charges_df['Arousal'] = charges_df['Charges'].apply(arousal_average)
charges_df['Dominance'] = charges_df['Charges'].apply(dominance_average)
```

```{python}
def combine_and_lower(x):
    return (' ').join([string.lower() for string in x])    
charges_df['Charges'].apply(combine_and_lower)
```
